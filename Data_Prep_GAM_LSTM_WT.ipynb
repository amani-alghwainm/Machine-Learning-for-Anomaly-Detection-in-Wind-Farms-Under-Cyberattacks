{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2efd48f",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ae472-760b-49ff-a98d-09d177ebcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb89767",
   "metadata": {},
   "source": [
    "### Load SCADA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d0755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input the base directory where Kelmarsh Dataset is located\n",
    "# base_directory = input(\"Enter the base directory for Kelmarsh Dataset: \")\n",
    "\n",
    "# Create file paths for the four Turbine_Data_Kelmarsh data files\n",
    "file_paths = [\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2018_3084/Turbine_Data_Kelmarsh_1_2018-01-01_-_2019-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2019_3085/Turbine_Data_Kelmarsh_1_2019-01-01_-_2020-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2020_3086/Turbine_Data_Kelmarsh_1_2020-01-01_-_2021-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2021_3087/Turbine_Data_Kelmarsh_1_2021-01-01_-_2021-07-01_228.csv\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read the data from the files and store in dataframes\n",
    "for file_path in file_paths:\n",
    "    WT_data = pd.read_csv(file_path, sep=',', skiprows=9)\n",
    "    dataframes.append(WT_data)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "WT_data = pd.concat(dataframes)\n",
    "WT_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a036be0",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "WT_data = WT_data.drop_duplicates()\n",
    "WT_data = WT_data.groupby(['Wind speed (m/s)', 'Power (kW)']).mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting Wind speed vs Power\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(data=WT_data, x='Wind speed (m/s)', y='Power (kW)')\n",
    "plt.title('Wind speed vs Power')\n",
    "\n",
    "# Plotting Rotor speed vs Power\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(data=WT_data, x='Rotor speed (RPM)', y='Power (kW)')\n",
    "plt.title('Rotor speed vs Power')\n",
    "\n",
    "# Plotting Wind speed vs Rotor speed\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.scatterplot(data=WT_data, x='Wind speed (m/s)', y='Rotor speed (RPM)')\n",
    "plt.title('Wind speed vs Rotor speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf21e6",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any rows with missing values\n",
    "# Selecting the columns to filter based on a regular expression pattern\n",
    "data_filter = WT_data.filter(regex='(count|Density|Index|Energy|Equivalent|Production|Cable|Voltage|factor|Current|Default|Virtual|Potential|potential|Avail|IEC|Lost|Max|Min|min|max|Std|Maximum|Minimum|deviation)')\n",
    "\n",
    "# Dropping the filtered columns and remove rows with missing values\n",
    "WT_data = WT_data.drop(data_filter.columns, axis=1)\n",
    "WT_data = WT_data.dropna(axis=0)\n",
    "\n",
    "# Converts the '# Date and time' column to datetime format\n",
    "WT_data['# Date and time'] = pd.to_datetime(WT_data['# Date and time'],format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Sort the dataset based on the '# Date and time' column\n",
    "WT_data = WT_data.sort_values('# Date and time')\n",
    "\n",
    "# Reset the index of the dataset\n",
    "WT_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79a825",
   "metadata": {},
   "source": [
    "### Label The Faults in The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Kelmarsh Turbine Status Data from multiple years\n",
    "file_paths = [\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2018_3084/Status_Kelmarsh_1_2018-01-01_-_2019-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2019_3085/Status_Kelmarsh_1_2019-01-01_-_2020-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2020_3086/Status_Kelmarsh_1_2020-01-01_-_2021-01-01_228.csv\",\n",
    "    f\"{base_directory}/Kelmarsh_SCADA_2021_3087/Status_Kelmarsh_1_2021-01-01_-_2021-07-01_228.csv\"\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    data_status = pd.read_csv(file_path, sep=',', skiprows=9)\n",
    "    # Removing invalid duration rows\n",
    "    data_status = data_status.drop(data_status[data_status['Duration'] == '-'].index)\n",
    "    # Reset the index of the dataset\n",
    "    data_status.reset_index(drop=True, inplace=True)\n",
    "    dataframes.append(data_status)\n",
    "\n",
    "data_status = pd.concat(dataframes)\n",
    "\n",
    "# Converts the 'Timestamp start' and 'Timestamp end' columns to datetime format\n",
    "data_status['Timestamp start'] = pd.to_datetime(data_status['Timestamp start'],format='%Y-%m-%d %H:%M:%S')\n",
    "data_status['Timestamp end'] = pd.to_datetime(data_status['Timestamp end'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef24bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743215a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting fault periods\n",
    "fault_periods = []\n",
    "for i, row in data_status.iterrows():\n",
    "    start = row['Timestamp start']\n",
    "    end = row['Timestamp end']\n",
    "    fault_periods.append((start, end))\n",
    "\n",
    "# Generating anomaly labels based on fault periods\n",
    "anomaly_labels = []\n",
    "for i, row in WT_data.iterrows():\n",
    "    timestamp = pd.to_datetime(row['# Date and time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    anomaly = 0\n",
    "    for start, end in fault_periods:\n",
    "        if start <= timestamp <= end:\n",
    "            anomaly = 1\n",
    "            break\n",
    "    anomaly_labels.append(anomaly)\n",
    "    \n",
    "# Converting the anomaly_classes list into a NumPy array\n",
    "anomaly_labels = np.array(anomaly_labels)\n",
    "\n",
    "print('Number of faults in the dataset :',anomaly_labels.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a42848",
   "metadata": {},
   "outputs": [],
   "source": [
    "WT_data = WT_data[['# Date and time', 'Rotor speed (RPM)', 'Wind speed (m/s)', 'Power (kW)']]\n",
    "WT_data['fault'] = anomaly_labels\n",
    "WT_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='Wind speed (m/s)', y='Power (kW)', hue=anomaly_labels, data=WT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a36ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='Wind speed (m/s)', y='Rotor speed (RPM)', hue=anomaly_labels, data=WT_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdce887",
   "metadata": {},
   "source": [
    "### Data Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import GAM\n",
    "\n",
    "# remove data with faults\n",
    "data_fit = WT_data[WT_data['fault']==0].copy()\n",
    "\n",
    "# Extract the relevant columns from the DataFrame\n",
    "wind_speed = data_fit['Wind speed (m/s)'].values\n",
    "power = data_fit['Power (kW)'].values\n",
    "rotor_speed = data_fit['Rotor speed (RPM)'].values\n",
    "\n",
    "# Fit a GAM for power\n",
    "gam_power = GAM(n_splines=20).fit(wind_speed, power)\n",
    "\n",
    "# Fit a GAM for rotor speed\n",
    "gam_rotor = GAM(n_splines=20).fit(wind_speed, rotor_speed)\n",
    "\n",
    "# Generate points for the fitted curves\n",
    "wind_speed_fit = np.linspace(wind_speed.min(), wind_speed.max(), 100)\n",
    "power_fit = gam_power.predict(wind_speed_fit)\n",
    "rotor_speed_fit = gam_rotor.predict(wind_speed_fit)\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(wind_speed, power, label='Data')\n",
    "ax1.plot(wind_speed_fit, power_fit, color='red', label='GAM Fit')\n",
    "ax1.set_xlabel('Wind Speed (m/s)', fontsize=14)\n",
    "ax1.set_ylabel('Power (kW)', fontsize=14)\n",
    "ax1.set_title('Wind Speed vs Power', fontsize=16)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "ax2.scatter(wind_speed, rotor_speed, label='Data')\n",
    "ax2.plot(wind_speed_fit, rotor_speed_fit, color='red', label='GAM Fit')\n",
    "ax2.set_xlabel('Wind Speed (m/s)', fontsize=14)\n",
    "ax2.set_ylabel('Rotor Speed (RPM)', fontsize=14)\n",
    "ax2.set_title('Wind Speed vs Rotor Speed', fontsize=16)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predict power and rotor speed using the fitted GAMs\n",
    "power_pred = gam_power.predict(wind_speed)\n",
    "rotor_speed_pred = gam_rotor.predict(wind_speed)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "power_mae = mean_absolute_error(power, power_pred)\n",
    "rotor_speed_mae = mean_absolute_error(rotor_speed, rotor_speed_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "power_rmse = np.sqrt(mean_squared_error(power, power_pred))\n",
    "rotor_speed_rmse = np.sqrt(mean_squared_error(rotor_speed, rotor_speed_pred))\n",
    "\n",
    "power_max_error = np.max(np.abs(power - power_pred))\n",
    "rotor_speed_max_error = np.max(np.abs(rotor_speed - rotor_speed_pred))\n",
    "\n",
    "print(f\"Power - Mean Absolute Error (MAE): {power_mae:.2f} kW\")\n",
    "print(f\"Power - Root Mean Squared Error (RMSE): {power_rmse:.2f} kW\")\n",
    "print(f\"Power - Maximum Error: {power_max_error:.2f} kW\")\n",
    "print(f\"Rotor Speed - Mean Absolute Error (MAE): {rotor_speed_mae:.2f} RPM\")\n",
    "print(f\"Rotor Speed - Root Mean Squared Error (RMSE): {rotor_speed_rmse:.2f} RPM\")\n",
    "print(f\"Rotor Speed - Maximum Error: {rotor_speed_max_error:.2f} RPM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a00e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(power_pred-power), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Power (kW)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of |Actual - Predicted| Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa16897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(rotor_speed_pred-rotor_speed), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Rotor Speed (RPM)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of |Actual - Predicted| Rotor Speed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44473131",
   "metadata": {},
   "source": [
    "### Simulating Cyberattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm\n",
    "import random\n",
    "\n",
    "np.random.seed(50)\n",
    "random.seed(50)\n",
    "\n",
    "def truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "def single_parameter_manipulation(data, column, start_index, duration, mean_error=0.7, error_range=(0.5, 0.9)):\n",
    "    end_index = start_index + duration\n",
    "    error_dist = truncated_normal(mean_error, 0.1, error_range[0], error_range[1])\n",
    "    error_factor = error_dist.rvs(duration + 1)\n",
    "    data.loc[start_index:end_index, column] = data.loc[start_index:end_index, column] * (1 + error_factor)\n",
    "    data.loc[start_index:end_index, 'Attack'] = 1\n",
    "    \n",
    "    return data, start_index, end_index\n",
    "\n",
    "def multiple_parameter_manipulation(data, columns, start_index, duration, mean_error=0.7, error_range=(0.5, 0.9)):\n",
    "    end_index = start_index + duration\n",
    "    for column in columns:\n",
    "        data, _, _ = single_parameter_manipulation(data, column, start_index, duration, mean_error, error_range)\n",
    "    return data, start_index, end_index\n",
    "\n",
    "def data_repetition(data, start_index, duration):\n",
    "    end_index = start_index + duration\n",
    "    repetition_start = start_index - duration - 1\n",
    "    repetition_end = start_index - 1\n",
    "    data.loc[start_index:end_index] = data.loc[repetition_start:repetition_end].values\n",
    "    data.loc[start_index:end_index, 'Attack'] = 1\n",
    "    return data, start_index, end_index\n",
    "\n",
    "def simulated_fault_type_1(data, column, start_index, duration):\n",
    "    end_index = start_index + duration\n",
    "    data.loc[start_index:end_index, column] = 0\n",
    "    data.loc[start_index:end_index, 'Attack'] = 1\n",
    "    return data, start_index, end_index\n",
    "\n",
    "def simulated_fault_type_2(data, column, start_index, duration, sd=0.5):\n",
    "    end_index = start_index + duration\n",
    "    error_dist = np.random.normal(0, sd, duration + 1)\n",
    "    data.loc[start_index:end_index, column] = data.loc[start_index:end_index, column] * (1 + error_dist)\n",
    "    data.loc[start_index:end_index, 'Attack'] = 1\n",
    "    return data, start_index, end_index\n",
    "\n",
    "def simulate_attacks(data, num_attacks=100, min_duration=6, max_duration=24, min_duration_fault=2, max_duration_fault=6):\n",
    "    manipulated_data = data.copy()\n",
    "    manipulated_data['Attack'] = 0\n",
    "    manipulated_data['Attack_Type'] = 0\n",
    "\n",
    "    attack_info_list = []\n",
    "    attack_number = 1\n",
    "\n",
    "    for attack_num in range(num_attacks):\n",
    "        print(f\"Attack {attack_num + 1}\")\n",
    "\n",
    "        attack_type = random.randint(1, 4) \n",
    "        print(f\"Attack Type: {attack_type}\")\n",
    "\n",
    "        if attack_type == 1:  # Single Parameter Manipulation\n",
    "            parameter = random.choice(['Rotor speed (RPM)', 'Wind speed (m/s)', 'Power (kW)'])\n",
    "            start_index = random.randint(0, len(manipulated_data) - max_duration)\n",
    "            duration = random.randint(min_duration, max_duration)\n",
    "            manipulated_data.loc[start_index:start_index + duration, 'Attack_Type'] = 1\n",
    "            manipulated_data, start_single, end_single = single_parameter_manipulation(manipulated_data, parameter, start_index, duration)\n",
    "            attack_info_list.append({\n",
    "                'Attack_Number': attack_number,\n",
    "                'Starting_Time': start_single,\n",
    "                'End_Time': end_single\n",
    "            })\n",
    "            attack_number += 1\n",
    "\n",
    "        elif attack_type == 2:  # Multiple Parameter Manipulation\n",
    "            num_parameters = random.randint(2, 3)\n",
    "            parameters = random.sample(['Rotor speed (RPM)', 'Wind speed (m/s)', 'Power (kW)'], k=num_parameters)\n",
    "            start_index = random.randint(0, len(manipulated_data) - max_duration)\n",
    "            duration = random.randint(min_duration, max_duration)\n",
    "            manipulated_data.loc[start_index:start_index + duration, 'Attack_Type'] = 2\n",
    "            manipulated_data, start_multiple, end_multiple = multiple_parameter_manipulation(manipulated_data, parameters, start_index, duration)\n",
    "            attack_info_list.append({\n",
    "                'Attack_Number': attack_number,\n",
    "                'Starting_Time': start_multiple,\n",
    "                'End_Time': end_multiple\n",
    "            })\n",
    "            attack_number += 1\n",
    "\n",
    "        elif attack_type == 3:  # Data Repetition\n",
    "            start_index = random.randint(max_duration, len(manipulated_data) - max_duration)  # Ensure enough data for repetition\n",
    "            duration = random.randint(min_duration, max_duration)\n",
    "            manipulated_data, start_repetition, end_repetition = data_repetition(manipulated_data, start_index, duration)\n",
    "            attack_info_list.append({\n",
    "                'Attack_Number': attack_number,\n",
    "                'Starting_Time': start_repetition,\n",
    "                'End_Time': end_repetition\n",
    "            })\n",
    "            manipulated_data.loc[start_index:start_index + duration, 'Attack_Type'] = 3\n",
    "            attack_number += 1\n",
    "\n",
    "        elif attack_type == 4:  # Simulated Fault Type 1\n",
    "            parameter = random.choice(['Rotor speed (RPM)', 'Wind speed (m/s)', 'Power (kW)'])\n",
    "            start_index = random.randint(0, len(manipulated_data) - max_duration_fault)\n",
    "            duration = random.randint(min_duration_fault, max_duration_fault)\n",
    "            manipulated_data.loc[start_index:start_index + duration, 'Attack_Type'] = 4\n",
    "            manipulated_data, start_fault1, end_fault1 = simulated_fault_type_1(manipulated_data, parameter, start_index, duration)\n",
    "            attack_info_list.append({\n",
    "                'Attack_Number': attack_number,\n",
    "                'Starting_Time': start_fault1,\n",
    "                'End_Time': end_fault1\n",
    "            })\n",
    "            attack_number += 1\n",
    "\n",
    "\n",
    "    # Convert list to DataFrame\n",
    "    attack_info = pd.DataFrame(attack_info_list)\n",
    "\n",
    "    return manipulated_data, attack_info\n",
    "\n",
    "# Perform cyber attacks\n",
    "min_duration = 6  # Minimum duration of 1 hour (assuming data is recorded every 10 minutes)\n",
    "max_duration = 24  # Maximum duration of 4 hours (assuming data is recorded every 10 minutes)\n",
    "min_duration_fault = 2  # Minimum duration of 20 minutes\n",
    "max_duration_fault = 6  # Maximum duration of 1 hour\n",
    "\n",
    "manipulated_data, attack_info = simulate_attacks(WT_data, num_attacks=100, min_duration=min_duration, max_duration=max_duration,\n",
    "                                                 min_duration_fault=min_duration_fault, max_duration_fault=max_duration_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eec15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3022244",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_data['Power_GAM'] = gam_power.predict(manipulated_data['Wind speed (m/s)'])\n",
    "manipulated_data['Rotor_Speed_GAM'] = gam_rotor.predict(manipulated_data['Wind speed (m/s)'])\n",
    "manipulated_data[manipulated_data['Attack']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with 'fault' equal to 1 and 'Power (kW)' less than 0\n",
    "manipulated_data_2 = manipulated_data[(manipulated_data['fault'] != 1) & (manipulated_data['Power (kW)'] >= 0)]\n",
    "\n",
    "# Reset index if you want consecutive integer indices\n",
    "manipulated_data_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate the absolute differences for Power and Rotor Speed\n",
    "manipulated_data_2['Power_GAM_diff'] = abs(manipulated_data_2['Power (kW)'] - manipulated_data_2['Power_GAM'])\n",
    "manipulated_data_2['Rotor_Speed_GAM_diff'] = abs(manipulated_data_2['Rotor speed (RPM)'] - manipulated_data_2['Rotor_Speed_GAM'])\n",
    "\n",
    "manipulated_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945b64b-e451-459b-80c8-cc05082ac485",
   "metadata": {},
   "source": [
    "## LSTM with GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc504a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import h5py\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year < 2021]\n",
    "test_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year == 2021]\n",
    "\n",
    "# Define the input features and target\n",
    "input_features = ['Wind speed (m/s)', 'Power_GAM_diff', 'Rotor_Speed_GAM_diff']\n",
    "target = 'Attack'\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train = train_data[input_features].values\n",
    "y_train = train_data[target].values\n",
    "X_test = test_data[input_features].values\n",
    "y_test = test_data[target].values\n",
    "\n",
    "# Scale the input features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the input data to include the sequence length\n",
    "sequence_length = 10\n",
    "X_train_reshaped = []\n",
    "y_train_reshaped = []\n",
    "for i in range(sequence_length, len(X_train_scaled)):\n",
    "    X_train_reshaped.append(X_train_scaled[i - sequence_length:i])\n",
    "    y_train_reshaped.append(y_train[i])  # Shift the labels by sequence_length\n",
    "X_train_reshaped = np.array(X_train_reshaped)\n",
    "y_train_reshaped = np.array(y_train_reshaped)\n",
    "\n",
    "X_test_reshaped = []\n",
    "y_test_reshaped = []\n",
    "for i in range(sequence_length, len(X_test_scaled)):\n",
    "    X_test_reshaped.append(X_test_scaled[i - sequence_length:i])\n",
    "    y_test_reshaped.append(y_test[i])  # Shift the labels by sequence_length\n",
    "X_test_reshaped = np.array(X_test_reshaped)\n",
    "y_test_reshaped = np.array(y_test_reshaped)\n",
    "\n",
    "# Build the LSTM model with additional layers and dropout\n",
    "lstm_GAM = Sequential()\n",
    "lstm_GAM.add(LSTM(128, input_shape=(sequence_length, len(input_features)), return_sequences=True))\n",
    "lstm_GAM.add(Dropout(0.2))\n",
    "lstm_GAM.add(LSTM(64, return_sequences=True))\n",
    "lstm_GAM.add(Dropout(0.2))\n",
    "lstm_GAM.add(LSTM(32))\n",
    "lstm_GAM.add(Dropout(0.2))\n",
    "lstm_GAM.add(Dense(1, activation='sigmoid'))\n",
    "lstm_GAM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "lstm_GAM.fit(X_train_reshaped, y_train_reshaped, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "lstm_GAM.save('lstm_GAM.h5')\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = lstm_GAM.evaluate(X_test_reshaped, y_test_reshaped)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (previous code remains the same)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = lstm_GAM.evaluate(X_test_reshaped, y_test_reshaped)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_prob = lstm_GAM.predict(X_test_reshaped)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_reshaped, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Attack'])\n",
    "plt.yticks(tick_marks, ['Normal', 'Attack'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add numbers to the confusion matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766215a-517c-461d-808d-c950eb8aa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the following variables defined:\n",
    "# test_data, sequence_length, y_pred\n",
    "\n",
    "# Calculate the number of rows to be removed based on the sequence length\n",
    "offset = sequence_length - 1  # This is the starting index for sequences\n",
    "\n",
    "# Ensure that you're considering the correct number of elements from the end\n",
    "test_data_with_predictions = test_data.iloc[offset:offset + len(y_pred)].copy()\n",
    "\n",
    "# Now add the predictions\n",
    "test_data_with_predictions['Predicted Attack'] = y_pred\n",
    "\n",
    "# Check to ensure the DataFrame and predictions align\n",
    "assert len(test_data_with_predictions) == len(y_pred), \"The lengths do not match.\"\n",
    "\n",
    "# Now, test_data_with_predictions includes the predictions aligned with the data\n",
    "\n",
    "# Filter for rows where actual attack is 1 and predicted attack is 0\n",
    "false_negatives = test_data_with_predictions[(test_data_with_predictions['Attack'] == 1) & (test_data_with_predictions['Predicted Attack'] == 0)]\n",
    "\n",
    "# Display these rows\n",
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20680ad1",
   "metadata": {},
   "source": [
    "### LSTM without GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year < 2021]\n",
    "test_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year == 2021]\n",
    "\n",
    "# Define the input features and target\n",
    "input_features = ['Wind speed (m/s)', 'Power (kW)', 'Rotor speed (RPM)']\n",
    "target = 'Attack'\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train = train_data[input_features].values\n",
    "y_train = train_data[target].values\n",
    "X_test = test_data[input_features].values\n",
    "y_test = test_data[target].values\n",
    "\n",
    "# Scale the input features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the input data to include the sequence length\n",
    "sequence_length = 10\n",
    "X_train_reshaped = []\n",
    "y_train_reshaped = []\n",
    "for i in range(sequence_length, len(X_train_scaled)):\n",
    "    X_train_reshaped.append(X_train_scaled[i - sequence_length:i])\n",
    "    y_train_reshaped.append(y_train[i])  # Shift the labels by sequence_length\n",
    "X_train_reshaped = np.array(X_train_reshaped)\n",
    "y_train_reshaped = np.array(y_train_reshaped)\n",
    "\n",
    "X_test_reshaped = []\n",
    "y_test_reshaped = []\n",
    "for i in range(sequence_length, len(X_test_scaled)):\n",
    "    X_test_reshaped.append(X_test_scaled[i - sequence_length:i])\n",
    "    y_test_reshaped.append(y_test[i])  # Shift the labels by sequence_length\n",
    "X_test_reshaped = np.array(X_test_reshaped)\n",
    "y_test_reshaped = np.array(y_test_reshaped)\n",
    "\n",
    "# Build the LSTM model with additional layers and dropout\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, input_shape=(sequence_length, len(input_features)), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(64, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "lstm_model.fit(X_train_reshaped, y_train_reshaped, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = lstm_model.evaluate(X_test_reshaped, y_test_reshaped)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "loss, accuracy = lstm_model.evaluate(X_test_reshaped, y_test_reshaped)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_prob = lstm_model.predict(X_test_reshaped)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_reshaped, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Attack'])\n",
    "plt.yticks(tick_marks, ['Normal', 'Attack'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add numbers to the confusion matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ae906",
   "metadata": {},
   "source": [
    "### Isolation forest with GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year < 2021]\n",
    "test_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year == 2021]\n",
    "\n",
    "# Define the input features and target\n",
    "input_features = ['Wind speed (m/s)', 'Power_GAM_diff', 'Rotor_Speed_GAM_diff']\n",
    "target = 'Attack'\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train = train_data[input_features].values\n",
    "y_train = train_data[target].values\n",
    "X_test = test_data[input_features].values\n",
    "y_test = test_data[target].values\n",
    "\n",
    "# Scale the input features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an Isolation Forest model\n",
    "IF_GAM = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "IF_GAM.fit(X_train_scaled)\n",
    "\n",
    "joblib.dump(IF_GAM, 'iforest_GAM.pkl')\n",
    "\n",
    "# Make predictions on the training and testing data\n",
    "y_train_pred = IF_GAM.predict(X_train_scaled)\n",
    "y_test_pred = IF_GAM.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions to binary labels (-1 for anomalies, 1 for normal)\n",
    "y_train_pred = np.where(y_train_pred == -1, 1, 0)\n",
    "y_test_pred = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "train_accuracy = (y_train_pred == y_train).mean()\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_accuracy = (y_test_pred == y_test).mean()\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "y_test_pred = IF_GAM.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions to binary labels (-1 for anomalies, 1 for normal)\n",
    "y_test_pred = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_accuracy = (y_test_pred == y_test).mean()\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Anomaly'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Normal', 'Anomaly'])\n",
    "\n",
    "# Add numbers to the confusion matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b48d2",
   "metadata": {},
   "source": [
    "### Isolation forest without GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c182e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year < 2021]\n",
    "test_data = manipulated_data_2[manipulated_data_2['# Date and time'].dt.year == 2021]\n",
    "\n",
    "# Define the input features and target\n",
    "input_features = ['Wind speed (m/s)', 'Power (kW)', 'Rotor speed (RPM)']\n",
    "target = 'Attack'\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train = train_data[input_features].values\n",
    "y_train = train_data[target].values\n",
    "X_test = test_data[input_features].values\n",
    "y_test = test_data[target].values\n",
    "\n",
    "# Scale the input features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an Isolation Forest model\n",
    "IF_model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "IF_model.fit(X_train_scaled)\n",
    "\n",
    "# Make predictions on the training and testing data\n",
    "y_train_pred = IF_model.predict(X_train_scaled)\n",
    "y_test_pred = IF_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions to binary labels (-1 for anomalies, 1 for normal)\n",
    "y_train_pred = np.where(y_train_pred == -1, 1, 0)\n",
    "y_test_pred = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "train_accuracy = (y_train_pred == y_train).mean()\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_accuracy = (y_test_pred == y_test).mean()\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "y_test_pred = IF_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions to binary labels (-1 for anomalies, 1 for normal)\n",
    "y_test_pred = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_accuracy = (y_test_pred == y_test).mean()\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Anomaly'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Normal', 'Anomaly'])\n",
    "\n",
    "# Add numbers to the confusion matrix\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
